{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, PolynomialFeatures,scale\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, LinearRegression,Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer,IterativeImputer\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/Users/racheltan/Desktop/QMSS/AML/winemag-data-130k-v2.csv\"\n",
    "path2 = \"/Users/ellen/Documents/GitHub/assignment-4-rachel_ellen/wine-reviews/winemag-data-130k-v2.csv\"\n",
    "\n",
    "wine_raw = pd.read_csv(path2, index_col = 0).drop(['taster_name', 'taster_twitter_handle'], axis = 1)\n",
    "\n",
    "#used the dataset with 130k rows\n",
    "#dropped the title since it contains mostly unique values and would be equivalent to and id column leaking information \n",
    "#dropped taster twitter handle and taster name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_usa = wine_raw[wine_raw.country == 'US']\n",
    "wine_usa_skew = wine_usa[wine_usa.price < 250].drop_duplicates('description') #using wine only from US, remove duplicate rows and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54504"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wine_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50170"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wine_usa_skew) #remove 300 rows of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_us = wine_usa_skew.sample(n = 20000, random_state = 123) #subsample 20k rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country           0\n",
       "description       0\n",
       "designation    6418\n",
       "points            0\n",
       "price             0\n",
       "province          0\n",
       "region_1        115\n",
       "region_2       1478\n",
       "title             0\n",
       "variety           0\n",
       "winery            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_us.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95049     240.0\n",
       "10747     235.0\n",
       "5025      230.0\n",
       "66940     225.0\n",
       "122558    225.0\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_us.price.nlargest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Create a baseline model for predicting wine quality using only non-text features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Feature: Points (indicating wine quality) \n",
    "\n",
    "Predictive Non-Text Features: \n",
    "- Designation \n",
    "- Price \n",
    "- Province \n",
    "- Region_1\n",
    "- Region_2\n",
    "- Variety\n",
    "- Winery \n",
    "\n",
    "We treat price as continuous and the rest as high cardinality categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (8, 4))\n",
    "points, price = axes.ravel()\n",
    "fig.suptitle('Distributions of Continuous Variables and Target', fontsize=15)\n",
    "axes[0].hist('points', data = wine_us, bins = 30)\n",
    "axes[0].set_title('Points')\n",
    "axes[0].set_xlabel('Points')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist('price', data = wine_us, bins = 30)\n",
    "axes[1].set_title('Price')\n",
    "axes[1].set_xlabel('Dollars')\n",
    "axes[1].set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_us[['designation', 'price', 'province', 'region_1', 'region_2', 'variety','winery']] #take out title \n",
    "y = wine_us['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "designation     object\n",
       "price          float64\n",
       "province        object\n",
       "region_1        object\n",
       "region_2        object\n",
       "variety         object\n",
       "winery          object\n",
       "title           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = X_train.dtypes == object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hi ellen - I tried to use target encoder but i keep getting a fit fail warning.. not sure what to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocess = make_pipeline(SimpleImputer(strategy='constant', fill_value='NA'), \n",
    "                              OneHotEncoder(handle_unknown = 'ignore')) #making a new category for NA values\n",
    "#target_preprocess = make_pipeline(SimpleImputer(strategy='constant', fill_value='NA'), #making a new category for NA values\n",
    "#                               TargetEncoder())\n",
    "cont_preprocess_scale = make_pipeline(IterativeImputer(estimator=RandomForestRegressor()), \n",
    "                                      StandardScaler())\n",
    "\n",
    "preprocess_scale = make_column_transformer(\n",
    "    (cat_preprocess, categorical),\n",
    "    #(target_preprocess, ['province']),\n",
    "    (cont_preprocess_scale, ~categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4078603329275129"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating our baseline model \n",
    "simple_model = make_pipeline(preprocess_scale, Ridge()) \n",
    "score_simple_model = cross_val_score(simple_model, X_train, y_train)\n",
    "np.mean(score_simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Create a simple text-based model using a bag-of-words approach and a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = wine_us[['description', 'title','points']] #just using the description column to predict - possibility of adding title??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description    0\n",
       "title          0\n",
       "points         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data.isnull().sum() #no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>This is a relatively thick and dense wine, gri...</td>\n",
       "      <td>Davis Family 2014 Soul Patch Estate Grown Pino...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116014</th>\n",
       "      <td>A routine, although entirely drinkable, Merlot...</td>\n",
       "      <td>Stonehedge 2010 Merlot (California)</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128693</th>\n",
       "      <td>Tremendously rich and oaky, with butterscotch-...</td>\n",
       "      <td>Dutton Estate 2007 Dutton Ranch Dutton Palms V...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94611</th>\n",
       "      <td>White pepper dominates the smell, and the flav...</td>\n",
       "      <td>Cambridge &amp; Sunset 2011 Sauvignon Blanc (Calif...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51328</th>\n",
       "      <td>This Yakima Valley bottling highlights fresh h...</td>\n",
       "      <td>Willow Crest 2009 Pinot Gris (Yakima Valley)</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  \\\n",
       "516     This is a relatively thick and dense wine, gri...   \n",
       "116014  A routine, although entirely drinkable, Merlot...   \n",
       "128693  Tremendously rich and oaky, with butterscotch-...   \n",
       "94611   White pepper dominates the smell, and the flav...   \n",
       "51328   This Yakima Valley bottling highlights fresh h...   \n",
       "\n",
       "                                                    title  points  \n",
       "516     Davis Family 2014 Soul Patch Estate Grown Pino...      91  \n",
       "116014                Stonehedge 2010 Merlot (California)      85  \n",
       "128693  Dutton Estate 2007 Dutton Ranch Dutton Palms V...      94  \n",
       "94611   Cambridge & Sunset 2011 Sauvignon Blanc (Calif...      81  \n",
       "51328        Willow Crest 2009 Pinot Gris (Yakima Valley)      90  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    wine_data['description'], wine_data['points'], stratify=wine_data['points'], random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81027    This is one of the best Paso Cabernets of the ...\n",
       "40963    Kenwood perfected their Chardonnay style a lon...\n",
       "41500    An interesting wine, grown in Cabernet country...\n",
       "28235    Consistency of richness and caramel toastiness...\n",
       "94045    A blend of 70% Grenache and 30% Mourv√®dre, thi...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical = X_text_train.dtypes == object\n",
    "#categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess = make_column_transformer(\n",
    "#    (CountVectorizer(), categorical))\n",
    "#simple_ridge = make_pipeline(preprocess, Ridge())\n",
    "#score_simple_ridge = cross_val_score(simple_ridge, X_text_train, y_text_train)\n",
    "#np.mean(score_simple_ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "my_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "my_stopwords.remove(\"well\")\n",
    "my_stopwords.remove(\"not\")\n",
    "my_stopwords.add(\"ve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5733086622189358"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just using the description column, and removing stop words \n",
    "simple_ridge = make_pipeline(CountVectorizer(stop_words = my_stopwords, min_df = 2), Ridge())\n",
    "score_simple_ridge = cross_val_score(simple_ridge, X_text_train, y_text_train)\n",
    "np.mean(score_simple_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW\n",
    "model. Be aware that you might need to adjust the (regularization of the) linear model for\n",
    "different feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the BoW model using stop words (altered to include negative terms), n-grams, tf-idf, min_df and max_features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to add some ngrams, min_df, stopwords to see if they improve the model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6285126796477959"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ngrams does help \n",
    "simple_ridge3 = make_pipeline(CountVectorizer(stop_words = my_stopwords, ngram_range = (1,3), min_df = 2), Ridge())\n",
    "score_simple_ridge3 = cross_val_score(simple_ridge3, X_text_train, y_text_train)\n",
    "np.mean(score_simple_ridge3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6558308786681517"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding tf_idf - not much difference \n",
    "tfidf_vect = TfidfVectorizer(stop_words = my_stopwords, ngram_range = (1,3), min_df = 2) #tfidf vectorizer is the same as count vec + tfidf transformer\n",
    "tfidf_ridge =  make_pipeline(tfidf_vect, Ridge())\n",
    "score_tfidf_ridge = cross_val_score(tfidf_ridge, X_text_train, y_text_train)\n",
    "np.mean(score_tfidf_ridge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.701294897488722"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using characters instead of words \n",
    "tfidf_vect = TfidfVectorizer(stop_words = my_stopwords, ngram_range = (2,5), min_df = 2, analyzer=\"char\") #tfidf vectorizer is the same as count vec + tfidf transformer\n",
    "tfidf_ridge =  make_pipeline(tfidf_vect, Ridge())\n",
    "score_tfidf_ridge = cross_val_score(tfidf_ridge, X_text_train, y_text_train)\n",
    "np.mean(score_tfidf_ridge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a grid search \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "param_grid = {\"ridge__alpha\": np.logspace(-3,3,9),\n",
    "              \"tfidfvectorizer__ngram_range\": [(2, 3), (2, 5), (3, 8), (5, 5)],\n",
    "              \"tfidfvectorizer__min_df\": [1, 2, 3]} #tfidf does normalizer by default\n",
    "grid = GridSearchCV(make_pipeline(TfidfVectorizer(stop_words = my_stopwords, analyzer=\"char\"), Ridge()),\n",
    "                    param_grid=param_grid, cv=5, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.27s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.23s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.26s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 16.49s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 15.50s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 16.04s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 16.42s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 16.09s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 137.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 132.97s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 136.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 138.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 142.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 9.80s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 10.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.90s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 10.99s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.47s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.92s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.99s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.94s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.89s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.96s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.01s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.23s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 11.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 67.68s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 67.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 69.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 73.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 64.89s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 7.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 7.66s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 8.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 7.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 7.46s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.72s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.95s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.96s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.83s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.96s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 9.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 9.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 9.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 9.99s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 9.52s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 58.55s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 51.81s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 51.90s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 52.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 51.91s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 4.88s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 5.06s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 6.74s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 6.32s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 6.01s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 10.86s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 1.0,\n",
       " 'tfidfvectorizer__min_df': 3,\n",
       " 'tfidfvectorizer__ngram_range': (2, 5)}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_text_train, y_text_train.values) #getting spurious output but so does the professor's gridsearch in the slides? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 1.0,\n",
       " 'tfidfvectorizer__min_df': 3,\n",
       " 'tfidfvectorizer__ngram_range': (2, 5)}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_ #should probably search through more parameters but it took forever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 1.0,\n",
       " 'tfidfvectorizer__min_df': 3,\n",
       " 'tfidfvectorizer__ngram_range': (2, 5)}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add another text column 'title' to see if this will give us better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train2, X_text_test2, y_text_train2, y_text_test2 = train_test_split(\n",
    "    wine_data[['description', 'title']], wine_data['points'], stratify=wine_data['points'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7470023347550149"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the parameters from the grid search above \n",
    "tfidf_vect2 = TfidfVectorizer(stop_words = my_stopwords, ngram_range = (2,5), min_df = 3, analyzer=\"char\")\n",
    "preprocess2 = make_column_transformer(    \n",
    "    (tfidf_vect2, 'description'), \n",
    "    (tfidf_vect2, 'title'))\n",
    "tfidf_ridge2 = make_pipeline(preprocess2, Ridge(alpha = 1.0))\n",
    "score_tfidf_ridge2 = cross_val_score(tfidf_ridge2, X_text_train2, y_text_train2)\n",
    "np.mean(score_tfidf_ridge2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Combine the non-text features and the text features. How does adding those features\n",
    "improve upon just using bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = wine_us[['description','designation', 'price', 'province', 'region_1', 'region_2', 'variety','winery', 'title']] #take out title \n",
    "y_full = wine_us['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train, X_full_test, y_full_train, y_full_test = train_test_split(X_full,y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description     True\n",
       "designation     True\n",
       "price          False\n",
       "province        True\n",
       "region_1        True\n",
       "region_2        True\n",
       "variety         True\n",
       "winery          True\n",
       "title           True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical = X_full_train.dtypes == object\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472972938800174"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_preprocess = make_pipeline(SimpleImputer(strategy='constant', fill_value='NA'), \n",
    "                              OneHotEncoder(handle_unknown = 'ignore')) #making a new category for NA values\n",
    "\n",
    "cont_preprocess_scale = make_pipeline(IterativeImputer(estimator=RandomForestRegressor()), \n",
    "                                      StandardScaler())\n",
    "\n",
    "preprocess = make_column_transformer(    \n",
    "    (cont_preprocess_scale, ~categorical), \n",
    "    (cat_preprocess, ['designation', 'province', 'region_1', 'region_2', 'variety', 'winery', 'title']),\n",
    "    (tfidf_vect2, 'description'), \n",
    "    (tfidf_vect2, 'title')) \n",
    "\n",
    "full_ridge = make_pipeline(preprocess, Ridge())\n",
    "score_full_ridge = cross_val_score(full_ridge, X_full_train, y_full_train)\n",
    "np.mean(score_full_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a gridsearch just to make sure that we are getting the best result. However from the initial results the non text features only seem to slightly improve the model results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocess = make_pipeline(SimpleImputer(strategy='constant', fill_value='NA'), \n",
    "                              OneHotEncoder(handle_unknown = 'ignore')) #making a new category for NA values\n",
    "\n",
    "cont_preprocess_scale = make_pipeline(IterativeImputer(estimator=RandomForestRegressor()), \n",
    "                                      StandardScaler())\n",
    "preprocess = make_column_transformer(    \n",
    "    (cont_preprocess_scale, ~categorical), \n",
    "    (cat_preprocess, ['designation', 'province', 'region_1', 'region_2', 'variety', 'winery', 'title']),\n",
    "    (TfidfVectorizer(stop_words = my_stopwords, analyzer=\"char\"), 'description'), \n",
    "    (TfidfVectorizer(stop_words = my_stopwords, analyzer=\"char\"), 'title')) \n",
    "\n",
    "ridge_model = Pipeline([('preprocess',preprocess), ('ridge',Ridge())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocess',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('pipeline-1',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('iterativeimputer',\n",
       "                                                                                          IterativeImputer(add_indicator=False,\n",
       "                                                                                                           estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                                                                                                           ccp_alpha=0...\n",
       "                         'preprocess__tfidfvectorizer-2__min_df': [2, 3],\n",
       "                         'preprocess__tfidfvectorizer-2__ngram_range': [(2, 3),\n",
       "                                                                        (2, 5),\n",
       "                                                                        (3, 8)],\n",
       "                         'ridge__alpha': array([1.00000000e-03, 5.62341325e-03, 3.16227766e-02, 1.77827941e-01,\n",
       "       1.00000000e+00, 5.62341325e+00, 3.16227766e+01, 1.77827941e+02,\n",
       "       1.00000000e+03])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_ridge = {\"ridge__alpha\": np.logspace(-3,3,9),\n",
    "              \"preprocess__tfidfvectorizer-1__ngram_range\": [(2, 3), (2, 5), (3, 8)],\n",
    "              \"preprocess__tfidfvectorizer-1__min_df\": [2, 3],\n",
    "              \"preprocess__tfidfvectorizer-2__ngram_range\": [(2, 3), (2, 5), (3, 8)],\n",
    "              \"preprocess__tfidfvectorizer-2__min_df\": [2, 3]} \n",
    "\n",
    "grid_ridge = GridSearchCV(ridge_model, param_grid=param_ridge,cv=5,return_train_score=True)\n",
    "grid_ridge.fit(X_full_train, y_full_train) \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7476805599580658"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ridge.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocess__tfidfvectorizer-1__min_df': 3,\n",
       " 'preprocess__tfidfvectorizer-1__ngram_range': (2, 5),\n",
       " 'preprocess__tfidfvectorizer-2__min_df': 3,\n",
       " 'preprocess__tfidfvectorizer-2__ngram_range': (2, 3),\n",
       " 'ridge__alpha': 1.0}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533639611160954"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ridge.score(X_full_test, y_full_test) #evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
